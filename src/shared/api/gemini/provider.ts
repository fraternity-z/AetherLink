import {
  GoogleGenAI,
  FinishReason
} from '@google/genai';
import type {
  Content
} from '@google/genai';
import type { Message, Model, MCPTool } from '../../types';
import { ChunkType } from '../../types/chunk';
import { getMainTextContent } from '../../utils/messageUtils';

import {
  isGemmaModel
} from '../../config/models';
import { takeRight } from 'lodash';
import { withRetry } from '../../utils/retryUtils';

import { GeminiConfigBuilder } from './configBuilder';
import { createGeminiEmbeddingService } from './embeddingService';
import { createGeminiMessageContentService } from './messageContentService';
import { fetchModels, createClient, testConnection } from './client';
import { createAbortController } from '../../utils/abortController';
import { GeminiStreamProcessor } from './streamProcessor';
import { GeminiCompletionService } from './completionService';





// æ¥å£å®šä¹‰ - å¤ç”¨ completionService ä¸­çš„ç±»å‹
export type { CompletionServiceParams as CompletionsParams } from './completionService';



// åŸºç¡€Providerç±»
export abstract class BaseProvider {
  protected model: Model;
  protected sdk: GoogleGenAI;

  constructor(model: Model) {
    this.model = model;
    this.sdk = createClient(model);
  }

  protected getAssistantSettings(assistant: any) {
    const maxTokens = Math.max(assistant?.maxTokens || assistant?.settings?.maxTokens || 4096, 1);
    const streamOutputValue = assistant?.settings?.streamOutput ?? assistant?.streamOutput;
    const streamOutput = streamOutputValue !== false;

    return {
      contextCount: assistant?.settings?.contextCount || 10,
      maxTokens: maxTokens,
      streamOutput: streamOutput
    };
  }



  protected createAbortController(messageId?: string, autoCleanup = false) {
    // ä½¿ç”¨ç»Ÿä¸€çš„ createAbortController å·¥å…·å‡½æ•°
    return createAbortController(messageId, autoCleanup);
  }

  protected async getMessageContent(message: Message): Promise<string> {
    return getMainTextContent(message);
  }

  // âœ¨ Phase 2-3 é‡æ„åæ¸…ç†ï¼š
  // - convertMcpTools: å·²ç§»åˆ° GeminiCompletionService
  // - setupToolsConfig: å·²åºŸå¼ƒï¼Œä½¿ç”¨ completionService.setupToolsAndPrompt
  // - useSystemPromptForTools: å·²åºŸå¼ƒ
  // - mcpToolCallResponseToMessage: å·²åºŸå¼ƒ
}



// Gemini Providerå®ç°
export default class GeminiProvider extends BaseProvider {
  private streamProcessor: GeminiStreamProcessor;
  private completionService: GeminiCompletionService;

  constructor(provider: any) {
    const model = {
      id: provider.models?.[0]?.id || 'gemini-pro',
      apiKey: provider.apiKey,
      baseUrl: provider.apiHost
    } as Model;
    super(model);
    this.streamProcessor = new GeminiStreamProcessor();
    this.completionService = new GeminiCompletionService(model);
  }





  /**
   * è·å–æ¶ˆæ¯å†…å®¹ - ä½¿ç”¨ä¸“é—¨çš„æ¶ˆæ¯å†…å®¹æœåŠ¡
   */
  private async getMessageContents(message: Message): Promise<Content> {
    const messageContentService = createGeminiMessageContentService(this.model);
    return messageContentService.getMessageContents(message);
  }

  // âœ¨ Phase 3: å·¥å…·è°ƒç”¨å¤„ç†å·²ç§»åˆ° completionService.ts
  // processGeminiFunctionCalls å’Œ processXMLToolCalls æ–¹æ³•å·²åˆ é™¤
  // ç°åœ¨ç”± completionService ç»Ÿä¸€å¤„ç†

  /**
   * æ ¸å¿ƒcompletionsæ–¹æ³• - å§”æ‰˜ç»™ completionService å¤„ç†
   *
   * ============= ç®€åŒ–åçš„è®¾è®¡ =============
   * 1. å‡†å¤‡æ¶ˆæ¯å†å² (completionService.prepareMessageHistory)
   * 2. é…ç½®å·¥å…·å’Œæç¤ºè¯ (completionService.setupToolsAndPrompt)
   * 3. æ„å»º API é…ç½® (GeminiConfigBuilder)
   * 4. æ‰§è¡Œå·¥å…·è°ƒç”¨å¾ªç¯ (completionService.executeToolCallLoop)
   */
  public async completions({
    messages,
    assistant,
    mcpTools,
    mcpMode = 'function',
    onChunk,
    onFilterMessages
  }: import('./completionService').CompletionServiceParams): Promise<void> {
    const model = assistant.model || this.model;
    const { maxTokens, streamOutput } = this.completionService.getAssistantSettings(assistant);

    // Step 1: å‡†å¤‡æ¶ˆæ¯å†å²
    const { history, userLastMessage } = await this.completionService.prepareMessageHistory(
      messages,
      assistant,
      onFilterMessages
    );

    // Step 2: é…ç½®å·¥å…·å’Œç³»ç»Ÿæç¤ºè¯
    const { systemInstruction, tools, usePromptMode } = this.completionService.setupToolsAndPrompt(
      assistant,
      mcpTools,
      mcpMode,
      model
    );

    // Step 3: æ„å»º API é…ç½®
    const configBuilder = new GeminiConfigBuilder(assistant, model, maxTokens, systemInstruction, tools);
    const generateContentConfig = configBuilder.build();

    // è°ƒè¯•æ—¥å¿—
    console.log(`[GeminiProvider.completions] ä½¿ç”¨ completionService:`, {
      mcpMode,
      usePromptMode,
      mcpToolsCount: mcpTools?.length || 0,
      toolsCount: tools.length,
      streamOutput,
      isGemmaModel: isGemmaModel(model)
    });

    // Step 4: è·å–æœ€åä¸€æ¡æ¶ˆæ¯å†…å®¹
    const messageContents: Content = await this.getMessageContents(userLastMessage!);

    // Step 5: åˆ›å»ºèŠå¤©ä¼šè¯
    const chat = this.sdk.chats.create({
      model: model.id,
      config: generateContentConfig,
      history: history
    });

    // Step 6: å¤„ç† Gemma æ¨¡å‹ç‰¹æ®Šæ ¼å¼
    this.completionService.handleGemmaFormat(model, assistant, messageContents, systemInstruction, history);

    // Step 7: åˆ›å»ºä¸­æ–­æ§åˆ¶å™¨
    const { cleanup, abortController } = this.createAbortController(userLastMessage?.id, true);

    try {
      // Step 8: æ‰§è¡Œå·¥å…·è°ƒç”¨å¾ªç¯
      await this.completionService.executeToolCallLoop({
        chat,
        messageContents,
        history,
        mcpTools,
        usePromptMode,
        streamOutput,
        generateContentConfig,
        onChunk,
        abortController,
        streamProcessor: this.streamProcessor
      });
    } finally {
      cleanup();
    }
  }





  /**
   * ç¿»è¯‘æ–¹æ³•
   */
  public async translate(
    content: string,
    assistant: any,
    onResponse?: (text: string, isComplete: boolean) => void,
    abortSignal?: AbortSignal
  ) {
    const model = assistant.model || this.model;
    const { maxTokens } = this.getAssistantSettings(assistant);

    const _content = isGemmaModel(model) && assistant.prompt
      ? `<start_of_turn>user\n${assistant.prompt}<end_of_turn>\n<start_of_turn>user\n${content}<end_of_turn>`
      : content;

    // ä½¿ç”¨ GeminiConfigBuilder æ„å»ºé…ç½®
    const configBuilder = new GeminiConfigBuilder(assistant, model, maxTokens, assistant.prompt, []);
    const config = configBuilder.build();

    if (!onResponse) {
      const response = await withRetry(
        () => this.sdk.models.generateContent({
          model: model.id,
          config: config,
          contents: [{ role: 'user', parts: [{ text: _content }] }]
        }),
        'Gemini Translate'
      );
      return response.text || '';
    }

    const response = await withRetry(
      () => this.sdk.models.generateContentStream({
        model: model.id,
        config: config,
        contents: [{ role: 'user', parts: [{ text: content }] }]
      }),
      'Gemini Translate Stream'
    );

    let text = '';
    for await (const chunk of response) {
      // æ£€æŸ¥ä¸­æ–­ä¿¡å·
      if (abortSignal?.aborted) {
        console.log('[GeminiProvider.translate] æµå¼å“åº”è¢«ç”¨æˆ·ä¸­æ–­');
        break;
      }

      text += chunk.text;
      onResponse?.(text, false);
    }
    onResponse?.(text, true);
    return text;
  }

  /**
   * ç”Ÿæˆæ‘˜è¦
   */
  public async summaries(messages: Message[], assistant: any): Promise<string> {
    const model = assistant.model || this.model;
    const userMessages = takeRight(messages, 5)
      .filter((message) => !message.isPreset)
      .map((message) => ({
        role: message.role,
        content: getMainTextContent(message)
      }));

    const userMessageContent = userMessages.reduce((prev, curr) => {
      const content = curr.role === 'user' ? `User: ${curr.content}` : `Assistant: ${curr.content}`;
      return prev + (prev ? '\n' : '') + content;
    }, '');

    const systemMessage = {
      role: 'system',
      content: 'è¯·ä¸ºä»¥ä¸‹å¯¹è¯ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ ‡é¢˜'
    };

    const userMessage = { role: 'user', content: userMessageContent };
    const content = isGemmaModel(model)
      ? `<start_of_turn>user\n${systemMessage.content}<end_of_turn>\n<start_of_turn>user\n${userMessage.content}<end_of_turn>`
      : userMessage.content;

    // ä½¿ç”¨ GeminiConfigBuilder æ„å»ºé…ç½®
    const configBuilder = new GeminiConfigBuilder(assistant, model, 4096, systemMessage.content, []);
    const config = configBuilder.build();

    const response = await this.sdk.models.generateContent({
      model: model.id,
      config: config,
      contents: [{ role: 'user', parts: [{ text: content }] }]
    });

    return response.text || '';
  }

  /**
   * ç”Ÿæˆæ–‡æœ¬
   */
  public async generateText({ prompt, content }: { prompt: string; content: string }): Promise<string> {
    const model = this.model;
    const MessageContent = isGemmaModel(model)
      ? `<start_of_turn>user\n${prompt}<end_of_turn>\n<start_of_turn>user\n${content}<end_of_turn>`
      : content;

    // åˆ›å»ºä¸´æ—¶åŠ©æ‰‹å¯¹è±¡ç”¨äºé…ç½®æ„å»º
    const tempAssistant = { prompt: prompt };

    // ä½¿ç”¨ GeminiConfigBuilder æ„å»ºé…ç½®
    const configBuilder = new GeminiConfigBuilder(tempAssistant, model, 4096, prompt, []);
    const config = configBuilder.build();

    const response = await this.sdk.models.generateContent({
      model: model.id,
      config: config,
      contents: [{ role: 'user', parts: [{ text: MessageContent }] }]
    });

    return response.text || '';
  }

  /**
   * ç”Ÿæˆå»ºè®®
   */
  public async suggestions(): Promise<any[]> {
    return [];
  }

  /**
   * æœç´¢æ‘˜è¦
   */
  public async summaryForSearch(messages: Message[], assistant: any): Promise<string> {
    const model = assistant.model || this.model;
    const systemMessage = { role: 'system', content: assistant.prompt };
    const userMessageContent = messages.map(getMainTextContent).join('\n');

    const content = isGemmaModel(model)
      ? `<start_of_turn>user\n${systemMessage.content}<end_of_turn>\n<start_of_turn>user\n${userMessageContent}<end_of_turn>`
      : userMessageContent;

    const lastUserMessage = messages[messages.length - 1];
    const { abortController, cleanup } = this.createAbortController(lastUserMessage?.id);
    const { signal } = abortController;

    // ä½¿ç”¨ GeminiConfigBuilder æ„å»ºé…ç½®
    const configBuilder = new GeminiConfigBuilder(assistant, model, 4096, systemMessage.content, []);
    const config = configBuilder.build();

    // æ·»åŠ ç‰¹å®šçš„é…ç½®é¡¹
    const finalConfig = {
      ...config,
      httpOptions: { timeout: 20 * 1000 },
      abortSignal: signal
    };

    const response = await this.sdk.models
      .generateContent({
        model: model.id,
        config: finalConfig,
        contents: [{ role: 'user', parts: [{ text: content }] }]
      })
      .finally(cleanup);

    return response.text || '';
  }

  /**
   * ç”Ÿæˆå›¾åƒ
   */
  public async generateImage(): Promise<string[]> {
    return [];
  }

  /**
   * æ£€æŸ¥æ¨¡å‹æœ‰æ•ˆæ€§
   */
  public async check(model: Model, stream: boolean = false): Promise<{ valid: boolean; error: Error | null }> {
    if (!model) {
      return { valid: false, error: new Error('No model found') };
    }

    // ä½¿ç”¨ GeminiConfigBuilder æ„å»ºé…ç½®ï¼Œä½†è®¾ç½®æœ€å°çš„ maxTokens ç”¨äºæµ‹è¯•
    const testAssistant = {
      enableThinking: false,
      thinkingBudget: 0
    };
    const configBuilder = new GeminiConfigBuilder(testAssistant, model, 1, undefined, []);
    const config = configBuilder.build();

    try {
      if (!stream) {
        const result = await this.sdk.models.generateContent({
          model: model.id,
          contents: [{ role: 'user', parts: [{ text: 'hi' }] }],
          config: config
        });
        if (!result.text) {
          throw new Error('Empty response');
        }
      } else {
        const response = await this.sdk.models.generateContentStream({
          model: model.id,
          contents: [{ role: 'user', parts: [{ text: 'hi' }] }],
          config: config
        });
        let hasContent = false;
        for await (const chunk of response) {
          if (chunk.candidates && chunk.candidates[0].finishReason === FinishReason.MAX_TOKENS) {
            hasContent = true;
            break;
          }
        }
        if (!hasContent) {
          throw new Error('Empty streaming response');
        }
      }
      return { valid: true, error: null };
    } catch (error: any) {
      return { valid: false, error };
    }
  }



  /**
   * è·å–æ–‡æœ¬åµŒå…¥
   */
  public async getEmbedding(
    text: string,
    options?: {
      taskType?: 'RETRIEVAL_QUERY' | 'RETRIEVAL_DOCUMENT' | 'SEMANTIC_SIMILARITY' | 'CLASSIFICATION' | 'CLUSTERING';
      title?: string;
    }
  ): Promise<number[]> {
    const embeddingService = createGeminiEmbeddingService(this.model);
    const result = await embeddingService.getEmbedding({
      text,
      model: this.model,
      taskType: options?.taskType,
      title: options?.title
    });
    return result.embedding;
  }

  /**
   * è·å–åµŒå…¥ç»´åº¦
   */
  public async getEmbeddingDimensions(model: Model): Promise<number> {
    const embeddingService = createGeminiEmbeddingService(model);
    return embeddingService.getEmbeddingDimensions(model);
  }

  /**
   * å…¼å®¹æ€§æ–¹æ³•ï¼šsendChatMessage - è½¬æ¢ä¸ºcompletionsè°ƒç”¨
   * 
   * ğŸ”§ ä¿®å¤ï¼šç›´æ¥è°ƒç”¨ completions æ–¹æ³•ï¼Œå¤ç”¨å·¥å…·å¤„ç†é€»è¾‘
   */
  public async sendChatMessage(
    messages: Message[],
    options?: {
      onChunk?: (chunk: any) => void;
      enableWebSearch?: boolean;
      enableThinking?: boolean;
      enableTools?: boolean;
      mcpTools?: MCPTool[];
      mcpMode?: 'prompt' | 'function';
      systemPrompt?: string;
      abortSignal?: AbortSignal;
      assistant?: any;
    }
  ): Promise<string | { content: string; reasoning?: string; reasoningTime?: number }> {
    // ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®å¤„ç† streamOutput è®¾ç½®
    // ä»ä¼ å…¥çš„ assistant ä¸­è¯»å–ï¼Œæ”¯æŒå¤šç§ä½ç½®
    const inputAssistant = options?.assistant;
    const streamOutputSetting = inputAssistant?.settings?.streamOutput ?? inputAssistant?.streamOutput;
    const streamOutput = streamOutputSetting !== false;
    
    console.log(`[GeminiProvider.sendChatMessage] streamOutput æ£€æµ‹:`, {
      'inputAssistant?.settings?.streamOutput': inputAssistant?.settings?.streamOutput,
      'inputAssistant?.streamOutput': inputAssistant?.streamOutput,
      'streamOutputSetting': streamOutputSetting,
      'streamOutput (final)': streamOutput
    });
    
    // æ„å»º assistant å¯¹è±¡
    const assistant = inputAssistant ? {
      ...inputAssistant,
      settings: {
        ...inputAssistant.settings,
        streamOutput: streamOutput  // ç¡®ä¿ streamOutput æ­£ç¡®è®¾ç½®
      }
    } : {
      model: this.model,
      prompt: options?.systemPrompt || '',
      settings: {
        streamOutput: streamOutput
      },
      enableWebSearch: options?.enableWebSearch || false,
      enableGenerateImage: false
    };

    // å¦‚æœæœ‰ä¼ å…¥çš„ assistant ä½†æ²¡æœ‰ promptï¼Œä½¿ç”¨ systemPrompt
    if (inputAssistant && options?.systemPrompt && !inputAssistant.prompt) {
      assistant.prompt = options.systemPrompt;
    }

    console.log(`[GeminiProvider.sendChatMessage] è°ƒç”¨ completions - mcpTools: ${options?.mcpTools?.length || 0}, mcpMode: ${options?.mcpMode || 'function'}, streamOutput: ${streamOutput}`);

    // æ”¶é›†å“åº”å†…å®¹
    let content = '';
    let reasoning = '';
    let reasoningTime = 0;

    // ğŸ”§ ä¿®å¤ï¼šç›´æ¥è°ƒç”¨ completions æ–¹æ³•ï¼Œå¤ç”¨æ‰€æœ‰å·¥å…·å¤„ç†é€»è¾‘
    await this.completions({
      messages,
      assistant,
      mcpTools: options?.mcpTools || [],
      mcpMode: options?.mcpMode || 'function',
      onChunk: (chunk: any) => {
        // è½¬å‘ chunk å¹¶æ”¶é›†å†…å®¹
        if (chunk.type === ChunkType.TEXT_DELTA) {
          content += chunk.text || '';
        } else if (chunk.type === ChunkType.TEXT_COMPLETE) {
          content = chunk.text || content;
        } else if (chunk.type === ChunkType.THINKING_DELTA) {
          reasoning += chunk.text || '';
        } else if (chunk.type === ChunkType.THINKING_COMPLETE) {
          reasoning = chunk.text || reasoning;
          reasoningTime = chunk.thinking_millsec || 0;
        }
        
        // è½¬å‘ç»™å¤–éƒ¨ onChunk
        options?.onChunk?.(chunk);
      },
      onFilterMessages: () => {}
    });

    return { content, reasoning, reasoningTime };
  }

  /**
   * å…¼å®¹æ€§æ–¹æ³•ï¼štestConnection
   */
  public async testConnection(): Promise<boolean> {
    return testConnection(this.model);
  }

  /**
   * å…¼å®¹æ€§æ–¹æ³•ï¼šgetModels
   */
  public async getModels(): Promise<any[]> {
    return fetchModels(this.model);
  }
}

// åŒæ—¶æä¾›å‘½åå¯¼å‡ºä»¥ç¡®ä¿å…¼å®¹æ€§
export { GeminiProvider };
